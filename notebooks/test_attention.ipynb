{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_device('cpu')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers_simple.transformer import MultiHeadAttention, TransformerBlock, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "input_size = 10\n",
    "input_vec_size = 7\n",
    "hidden_size = 5\n",
    "output_size = 4\n",
    "num_heads = 6\n",
    "vocab_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultiHeadAttention(input_size=input_vec_size, hidden_size=hidden_size,\n",
    "                        output_size=output_size, num_heads=num_heads, block_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(N, input_size, input_vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = net(torch.Tensor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TransformerBlock(block_size=input_size, vec_size=input_vec_size, hidden_size=20,\n",
    "                      attn_hidden_size=hidden_size, output_size=input_vec_size, num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.Tensor(X)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GPT(vocab_size=vocab_size, block_size=input_size, embed_size=15, hidden_size=10,\n",
    "          attn_hidden_size=hidden_size,\n",
    "         output_size=output_size, num_transformer_blocks=3, num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(vocab_size, size=(N,input_size)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8119, -0.1798, -0.6446,  0.0348],\n",
       "         [-0.8119, -0.1798, -0.6446,  0.0348],\n",
       "         [ 0.2825, -0.4206,  0.0581,  0.2651],\n",
       "         ...,\n",
       "         [-0.7709, -0.1832, -0.6729,  0.0308],\n",
       "         [-0.1002, -0.0596,  0.1096,  0.4100],\n",
       "         [-0.0668,  0.0986, -0.7228,  0.0859]],\n",
       "\n",
       "        [[ 0.2277, -0.4700,  0.1812,  0.2434],\n",
       "         [-0.1163, -0.1401,  0.2037,  0.3811],\n",
       "         [-0.0579,  0.0825, -0.7218,  0.0654],\n",
       "         ...,\n",
       "         [ 0.7911, -0.7016, -1.0886, -0.5459],\n",
       "         [ 0.1840, -0.0432, -0.2201, -0.1755],\n",
       "         [ 0.7872, -0.7054, -1.0912, -0.5477]],\n",
       "\n",
       "        [[ 0.6461,  0.5759,  0.2482, -0.3920],\n",
       "         [-0.0420,  0.0965, -0.7498,  0.0734],\n",
       "         [-0.0550,  0.1011, -0.7449,  0.0722],\n",
       "         ...,\n",
       "         [ 0.5793,  0.6237,  0.2578, -0.3776],\n",
       "         [ 0.2993, -0.4274, -0.0021,  0.2669],\n",
       "         [ 0.5899,  0.6082,  0.2649, -0.3636]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2277, -0.4700,  0.1812,  0.2434],\n",
       "         [-0.5850, -0.3053,  0.5086, -0.1199],\n",
       "         [-0.5644, -0.3020,  0.4545, -0.1122],\n",
       "         ...,\n",
       "         [-0.5675, -0.3278,  0.4759, -0.1288],\n",
       "         [ 0.6010,  0.5854,  0.2681, -0.3766],\n",
       "         [-0.0934, -0.0853,  0.1773,  0.4076]],\n",
       "\n",
       "        [[-0.1133, -0.1246,  0.1861,  0.3460],\n",
       "         [-0.5797, -0.3740,  0.5182, -0.0834],\n",
       "         [ 0.3068, -0.4570,  0.0954,  0.2207],\n",
       "         ...,\n",
       "         [-0.0512,  0.0795, -0.7331,  0.0685],\n",
       "         [ 0.7860, -0.7267, -1.0766, -0.5445],\n",
       "         [-0.0512,  0.0871, -0.7349,  0.0707]],\n",
       "\n",
       "        [[ 0.2277, -0.4700,  0.1812,  0.2434],\n",
       "         [-0.5850, -0.3053,  0.5086, -0.1199],\n",
       "         [ 0.6051,  0.5758,  0.2700, -0.3611],\n",
       "         ...,\n",
       "         [-0.0922, -0.1138,  0.1919,  0.3922],\n",
       "         [ 0.1790, -0.0631, -0.1938, -0.1886],\n",
       "         [-0.0879, -0.1190,  0.1865,  0.3877]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.IntTensor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers_simple]",
   "language": "python",
   "name": "conda-env-transformers_simple-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
